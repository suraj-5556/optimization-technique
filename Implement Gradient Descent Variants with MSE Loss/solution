import numpy as np

def gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):

    def feed_forward(array, weight):
        return np.dot(array, weight)
    
    def update_batch(weight, array, y):
        w=np.zeros(len(array[0]))
        for i in range(0,len(weight)):
            for j in range(0,len(y)):
                w[i] = w[i] + ((feed_forward(array[j], weight) - y[j]) * array[j][i])
        for i in range (0,len(weight)):
            weight[i]-= learning_rate*2*(w[i]/len(y))
    
    def update_s(weight,array,y):
        
        for i in range (0,len(y)):
            w=np.zeros(len(weight))
            for j in range (0,len(weight)):
                w[j]= (feed_forward(array[i], weight)-y[i])*array[i][j]
            for k in range (0,len(weight)):
                weight[k]=weight[k] -learning_rate*2*(w[k])
    
    def update_mini(weight, array, y,size):
        
        w=np.zeros(len(weight))
        count=0
        
        for i in range (0,len(y)):
            for j in range (0,len(weight)):
                w[j]= w[j]+(feed_forward(array[i], weight)-y[i])*array[i][j]
            count=count+1
            if count==size:
                for k in range (0,len(weight)):
                    weight[k]=weight[k] -learning_rate*2*(w[k]/size)
                w=np.zeros(len(weight))
                count=0
          


    if method == 'batch':
        for i in range (0,n_iterations):
            update_batch(weights,X,y)
    elif method == 'stochastic':
        for i in range (0,n_iterations):
            update_s(weights,X,y)
    elif method == 'mini_batch':
        for i in range (0,n_iterations):
            update_mini(weights,X,y,batch_size)
    return weights
    
