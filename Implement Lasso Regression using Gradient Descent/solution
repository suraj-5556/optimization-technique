import numpy as np

def l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:
	n_samples, n_features = X.shape

	weights = np.zeros(n_features)
	b = 0
	
    for i in range (0,max_iter):
        y_pred=np.dot(X,weights)+b
        count=0

        grad=np.zeros(n_features)

        for j in range (0,len(y)):
            for k in range (0,len(grad)):
                grad[k]=grad[k]+(y_pred[j]-y[j])*X[j][k]
        
        for k in range (0,len(weights)):
            weights[k]=weights[k]-learning_rate*((grad[k]/len(y)) + alpha*np.sign(weights[k]))

        b=b-learning_rate*(sum(y_pred)-sum(y))/len(y)
    return weights,b

